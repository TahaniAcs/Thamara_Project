import os
import json
import tensorflow as tf
import kagglehub
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# ==========================
# 1) Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª (Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ)
# ==========================

print("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø± KaggleHub...")
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙƒØ§Ø´
path = kagglehub.dataset_download("leftin/fruit-ripeness-unripe-ripe-and-rotten")
print(f"âœ… Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {path}")

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§ÙƒØªØ´Ø§Ù Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ­ÙŠØ­ (Ù„Ø£Ù† Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ ÙŠÙƒÙˆÙ† Ø¯Ø§Ø®Ù„ archive Ø£Ùˆ dataset)
possible_paths = [
    os.path.join(path, "fruit_ripeness_dataset", "archive (1)", "dataset"),
    os.path.join(path, "fruit_ripeness_dataset", "dataset"),
    os.path.join(path, "dataset"),
    path
]

BASE_DATA_DIR = None
for p in possible_paths:
    if os.path.exists(os.path.join(p, "train")):
        BASE_DATA_DIR = p
        break

if BASE_DATA_DIR is None:
    print("âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù„Ø¯ 'train'. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
    raise SystemExit

TRAINING_DIR = os.path.join(BASE_DATA_DIR, "train")
VALIDATION_DIR = os.path.join(BASE_DATA_DIR, "test")

print(f"ğŸ“‚ Ù…Ø³Ø§Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {TRAINING_DIR}")
print(f"ğŸ“‚ Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {VALIDATION_DIR}")

# Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
IMAGE_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS     = 50 # ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø¯ØªÙ‡Ø§ Ø¥Ù„Ù‰ 50

tf.random.set_seed(42)

# ==========================
# 2) ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================

train_datagen = ImageDataGenerator(
    rescale=1.0 / 255.0,
    rotation_range=20,
    horizontal_flip=True,
    fill_mode="nearest"
)

validation_datagen = ImageDataGenerator(rescale=1.0 / 255.0)

print("â³ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±...")

train_generator = train_datagen.flow_from_directory(
    TRAINING_DIR,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=True
)

validation_generator = validation_datagen.flow_from_directory(
    VALIDATION_DIR,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

# Ø­ÙØ¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰)
class_indices = train_generator.class_indices
NUM_CLASSES = len(class_indices)
print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª: {NUM_CLASSES}")

with open("class_indices.json", "w", encoding="utf-8") as f:
    json.dump(class_indices, f, ensure_ascii=False, indent=2)
print("âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù class_indices.json")

# ==========================
# 3) Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# ==========================

model = Sequential([
    Conv2D(32, (3, 3), activation="relu", input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation="relu"),
    MaxPooling2D(2, 2),
    Conv2D(128, (3, 3), activation="relu"),
    MaxPooling2D(2, 2),
    Flatten(),
    Dropout(0.5),
    Dense(128, activation="relu"),
    Dense(NUM_CLASSES, activation="softmax")
])

# ==========================
# 4) Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø­ÙØ¸
# ==========================

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    "thamara_ripeness_best.keras",
    monitor="val_loss",
    save_best_only=True,
    verbose=1
)

print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=validation_generator,
    callbacks=[checkpoint],
    verbose=1
)

model.save("thamara_ripeness_best.keras")
print("\nğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ ÙˆØ­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬!")
print("ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­.")
